{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Iterable, Optional, Tuple, Union\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(name: str):\n",
    "    if any(x in name for x in ('dino', 'mocov3', 'convnext', )):\n",
    "        normalize = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(name: str):\n",
    "    if 'dinov2' in name:\n",
    "        #  dinov2 models like dinov2_vits14\n",
    "        model = torch.hub.load('facebookresearch/dinov2:main', name)\n",
    "        model.fc = torch.nn.Identity()\n",
    "        val_transform = get_transform(name)\n",
    "        patch_size = model.patch_embed.patch_size\n",
    "        num_heads = model.blocks[0].attn.num_heads\n",
    "    elif 'dino' in name:\n",
    "        model = torch.hub.load('facebookresearch/dino:main', name)\n",
    "        model.fc = torch.nn.Identity()\n",
    "        val_transform = get_transform(name)\n",
    "        patch_size = model.patch_embed.patch_size\n",
    "        num_heads = model.blocks[0].attn.num_heads\n",
    "    else:\n",
    "        raise ValueError(f'Cannot get model: {name}')\n",
    "    model = model.eval()\n",
    "    return model, val_transform, patch_size, num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dinov2_vits14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Tmenova/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "model, val_transform, patch_size, num_heads = get_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_block = -1\n",
    "if 'dino' in model_name or 'mocov3' in model_name:\n",
    "    feat_out = {}\n",
    "    def hook_fn_forward_qkv(module, input, output):\n",
    "        feat_out[\"qkv\"] = output\n",
    "    model._modules[\"blocks\"][which_block]._modules[\"attn\"]._modules[\"qkv\"].register_forward_hook(hook_fn_forward_qkv)\n",
    "else:\n",
    "    raise ValueError(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDataset(Dataset):\n",
    "    \"\"\"A very simple dataset for loading images.\"\"\"\n",
    "\n",
    "    def __init__(self, filenames: str, images_root: Optional[str] = None, transform: Optional[Callable] = None,\n",
    "                 prepare_filenames: bool = True) -> None:\n",
    "        self.root = None if images_root is None else Path(images_root)\n",
    "        self.filenames = sorted(list(set(filenames))) if prepare_filenames else filenames\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        path = self.filenames[index]\n",
    "        full_path = Path(path) if self.root is None else self.root / path\n",
    "        assert full_path.is_file(), f'Not a file: {full_path}'\n",
    "        image = Image.open(full_path).convert('RGB')\n",
    "        # image =  \n",
    "        # image = cv2.imread(str(full_path))\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, path, index\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list = \"C:/Users/Tmenova/personal/tum/thesis/thesis-codebase/data/MINI/CAROTID_MIXED/val_mini/lists/images.txt\"\n",
    "images_root = \"C:/Users/Tmenova/personal/tum/thesis/thesis-codebase/data/MINI/CAROTID_MIXED/val_mini/images\"\n",
    "filenames = Path(images_list).read_text().splitlines()\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = ImagesDataset(filenames=filenames, images_root=images_root, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerator device= cuda\n"
     ]
    }
   ],
   "source": [
    "# Prepare\n",
    "cpu = True\n",
    "if torch.cuda.is_available():\n",
    "    cpu = False\n",
    "accelerator = Accelerator(cpu, mixed_precision=\"fp16\")\n",
    "# model, dataloader = accelerator.prepare(model, dataloader)\n",
    "model = model.to(accelerator.device)\n",
    "print('accelerator device=', accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(dataloader, desc='Processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0745, 0.0745],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0314, 0.0314, 0.0314],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0745, 0.0745],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0314, 0.0314, 0.0314],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0745, 0.0745],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0314, 0.0314, 0.0314],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]),\n",
       " 'img1042.png',\n",
       " 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, files, indices = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0745, 0.0745],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0314, 0.0314, 0.0314],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0745, 0.0745],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0314, 0.0314, 0.0314],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0745, 0.0745],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0314, 0.0314, 0.0314],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]),\n",
       " 'img1042.png',\n",
       " 0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, files, indices = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x00000272A914E648>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 10/10 [00:08<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(dataloader, desc='Processing')\n",
    "print(dataloader)\n",
    "for i, (images, files, indices) in enumerate(pbar):\n",
    "    output_dict = {}\n",
    "    # Reshape image\n",
    "    P = patch_size[0]\n",
    "    B, C, H, W = images.shape\n",
    "    H_patch, W_patch = H // P, W // P\n",
    "    H_pad, W_pad = H_patch * P, W_patch * P\n",
    "    T = H_patch * W_patch + 1  # number of tokens, add 1 for [CLS]\n",
    "    # images = F.interpolate(images, size=(H_pad, W_pad), mode='bilinear')  # resize image\n",
    "    images = images[:, :, :H_pad, :W_pad]\n",
    "    images = images.to(accelerator.device)\n",
    "\n",
    "    # collect features\n",
    "    if 'dinov2' in model_name or 'mocov3' in model_name:\n",
    "        with torch.no_grad():\n",
    "            # accelerator.unwrap_model(model).get_intermediate_layers(images)[0].squeeze(0)\n",
    "            model.get_intermediate_layers(images)[0].squeeze(0)\n",
    "        # output_dict['out'] = out\n",
    "        output_qkv = feat_out[\"qkv\"].reshape(B, T, 3, num_heads, -1 // num_heads).permute(2, 0, 3, 1, 4)\n",
    "        output_dict['k'] = output_qkv[1].transpose(1, 2).reshape(B, T, -1)[:, 1:, :]\n",
    "    else:\n",
    "        raise ValueError(model_name)\n",
    "    output_dict = {k: (v.detach().cpu() if torch.is_tensor(v) else v) for k, v in output_dict.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': tensor([[[ 1.8729,  0.2479, -1.4329,  ...,  0.1663, -0.1796, -1.0883],\n",
       "          [ 2.4307, -0.3700, -0.2780,  ...,  0.3730, -0.2587, -1.9577],\n",
       "          [ 1.0429,  0.3578, -1.9638,  ...,  0.1203, -0.3740, -0.9415],\n",
       "          ...,\n",
       "          [ 2.0477,  0.8199, -1.6675,  ..., -0.0553, -1.1283, -2.5139],\n",
       "          [ 1.9789,  0.3190, -0.7145,  ...,  0.3181, -0.6036, -0.5634],\n",
       "          [ 1.6825,  0.9970, -1.1676,  ..., -0.0787, -0.8086, -1.7645]]])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    feats = output_dict[\"k\"].squeeze().cuda()\n",
    "else:\n",
    "    feats = output_dict[\"k\"].squeeze().cpu()\n",
    "\n",
    "feats = F.normalize(feats, p=2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_numpy = feats.cpu().detach().numpy()\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=1)\n",
    "clusters1 = kmeans.fit_predict(feats_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict['indices'] = indices[0]\n",
    "output_dict['file'] = files[0]\n",
    "output_dict['id'] = id\n",
    "output_dict['model_name'] = model_name\n",
    "output_dict['patch_size'] = patch_size\n",
    "output_dict['shape'] = (B, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_segmap(clusters, data_dict):\n",
    "    B, C, H, W, P, H_patch, W_patch, H_pad, W_pad = get_image_sizes(data_dict)\n",
    "    # Reshape\n",
    "    infer_bg_index = True\n",
    "    if clusters.size == H_patch * W_patch:  # TODO: better solution might be to pass in patch index\n",
    "        segmap = clusters.reshape(H_patch, W_patch)\n",
    "    elif clusters.size == H_patch * W_patch * 4:\n",
    "        segmap = clusters.reshape(H_patch * 2, W_patch * 2)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "    # TODO: Improve this step in the pipeline.\n",
    "    # Background detection: we assume that the segment with the most border pixels is the \n",
    "    # background region. We will always make this region equal 0. \n",
    "    if infer_bg_index:\n",
    "        indices, normlized_counts = get_border_fraction(segmap)\n",
    "        bg_index = indices[np.argmax(normlized_counts)].item()\n",
    "        bg_region = (segmap == bg_index)\n",
    "        zero_region = (segmap == 0)\n",
    "        segmap[bg_region] = 0\n",
    "        segmap[zero_region] = bg_index\n",
    "\n",
    "    return segmap\n",
    "\n",
    "def get_image_sizes(data_dict: dict, downsample_factor = None):\n",
    "    P = data_dict['patch_size'] if downsample_factor is None else downsample_factor\n",
    "    P = P[0]\n",
    "    B, C, H, W = data_dict['shape']\n",
    "    assert B == 1, 'assumption violated :('\n",
    "    H_patch, W_patch = H // P, W // P\n",
    "    H_pad, W_pad = H_patch * P, W_patch * P\n",
    "    return (B, C, H, W, P, H_patch, W_patch, H_pad, W_pad)\n",
    "\n",
    "def get_border_fraction(segmap: np.array):\n",
    "    num_border_pixels = 2 * (segmap.shape[0] + segmap.shape[1])\n",
    "    counts_map = {idx: 0 for idx in np.unique(segmap)}\n",
    "    np.zeros(len(np.unique(segmap)))\n",
    "    for border in [segmap[:, 0], segmap[:, -1], segmap[0, :], segmap[-1, :]]:\n",
    "        unique, counts = np.unique(border, return_counts=True)\n",
    "        for idx, count in zip(unique.tolist(), counts.tolist()):\n",
    "            counts_map[idx] += count\n",
    "    # normlized_counts_map = {idx: count / num_border_pixels for idx, count in counts_map.items()}\n",
    "    indices = np.array(list(counts_map.keys()))\n",
    "    normlized_counts = np.array(list(counts_map.values())) / num_border_pixels\n",
    "    return indices, normlized_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x273701e0588>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "segmap_kmeans = get_segmap(clusters1, output_dict)\n",
    "plt.imshow(segmap_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loading other dinov2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Tmenova/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"dino_vits8\"\n",
    "# model_name = \"dino_vitb8\"\n",
    "# model_name = \"dinov2_vits14\"\n",
    "model_name = \"dinov2_vitb14\"\n",
    "\n",
    "\n",
    "model, val_transform, patch_size, num_heads = get_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
